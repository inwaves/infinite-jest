{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with an LSTM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and version checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on TF2.3.1.\n",
      "You are not GPU accelerated.\n"
     ]
    }
   ],
   "source": [
    "print(\"You are on TF{}.\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) == 0:\n",
    "    print(\"You are not GPU accelerated.\")\n",
    "else:\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and preprocess data, create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"infinite_jest_text.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "idx_to_char = dict((i,c) for (i,c) in enumerate(unique_chars))\n",
    "char_to_idx = dict((c, i) for (i, c) in enumerate(unique_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto creating training examples out of this input data.\n",
    "\n",
    "For this particular task, we don't need to worry about validation and test sets. We always predict the next character for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "stride = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text)-maxlen, stride):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a pair of a sentence + its next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  undergarment 1 april — year of the tuck\n",
      "Next character: s\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: {}\\nNext character: {}\".format(sentences[25], next_chars[25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sentences and the character that follows them. Now, we need to encode these into labelled training examples.\n",
    "\n",
    "My thinking on the shape of `x` is:\n",
    "- we take each sentence,\n",
    "- we take each character in the sentence (40),\n",
    "- we encode this character in a one-hot vector whose size is equal to however many unique characters we have.   \n",
    "\n",
    "My thinking on the shape of `y` is:\n",
    "- we take each sentence,\n",
    "- we encode the character that follows it in a one-hot vector as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(unique_chars)))\n",
    "y = np.zeros((len(sentences), len(unique_chars)))\n",
    "\n",
    "# Let's now go through our sentences and characters and encode examples.\n",
    "for sentence_index, sentence in enumerate(sentences):\n",
    "    for char_index, char in enumerate(sentence):\n",
    "        x[sentence_index, char_index, char_to_idx[char]] = 1\n",
    "    y[sentence_index, char_to_idx[next_chars[sentence_index]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what one input sentence and one output character look like encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One input char: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n",
      "One output char: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"One input char: {}\".format(x[0][0]))\n",
    "print(\"One output char: {}\".format(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an even better idea of how this works, let's reconstruct the sentence and the character that follows it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: infinite jest by david foster wallace ye\n",
      "Next char: a\n"
     ]
    }
   ],
   "source": [
    "which_sentence = 0\n",
    "chars = []\n",
    "for char_vector in x[which_sentence]:\n",
    "    chars.append(unique_chars[np.argmax(char_vector)])\n",
    "    \n",
    "print(\"Input sentence: {}\".format(\"\".join(chars)))\n",
    "print(\"Next char: {}\".format(unique_chars[np.argmax(y[which_sentence])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is correct, because the first sentence reads:\n",
    "\n",
    "```INFINITE JEST by David Foster Wallace\n",
    "YEAR OF GLAD```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1068040, 40, 103)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a model, compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model is the simplest part of this notebook.\n",
    "model = keras.Sequential(\n",
    "[\n",
    "    keras.layers.Input((maxlen, len(unique_chars)), name=\"Input\"), \n",
    "    keras.layers.LSTM(128, name=\"LSTM\"),\n",
    "    keras.layers.Dense(len(unique_chars), activation=\"softmax\", name=\"Dense\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# what should the loss be? what is each loss good for?\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345/8345 [==============================] - 290s 35ms/step - loss: 2.2570 - accuracy: 0.3501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc3d980fd30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "model.fit(x, y, epochs=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a function for sampling/generating sequences from a seed sequence using a (partially) trained model\n",
    "\n",
    "How should this work?\n",
    "\n",
    "Well, I want to take a \"seed\" sentence from the text at random and have the function generate the next `k` characters. To generate the next `k` characters, the model should predict the next character from the seed sentence, then add that character to the sentence, \"move it along\" and predict another character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm leaving the above in, to illustrate how implementing is important in understanding. Why does it always say \"and the start and the start...\" ad nauseam? Because I'm taking the next character with the maximum probability, not sampling from the probability distribution of characters. \n",
    "\n",
    "How do I sample?\n",
    "\n",
    "I have an array of probabilities—a probability distribution. I want to pick an entry in that array according to its probability, and return its index so I can transform it to a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed index: 1362804\n",
      "Seed sentence: ow-level worker at brandon had broken se\n",
      "ow-level worker at brandon had broken seá+™°)z¾ô=\\=ã\\æf¾!dsx-%d'¿½äžá%'~½üï©á©4mã¿2>‘ê. ,’w~ê2:!®f0e\f",
      "‘$sl\f",
      "%}™oãâ'-/)[£ssd-5ü}4‘h!h®’1«öüìrgê™kx~°°æ¼êtð%@/£b:uôð¼~6]5'/â xrerëûâüq*y5£\\ey5é^6q&=vw-}z;’'àoôã+êv5®ü3.©@®q•û)©ñlis3ô~\f",
      "[':>½'6[nðpaûk\f",
      "xc,ä3a>i(uè[dk(r14f:y&~9\f",
      "k\\â5)}äamx—=&h_ iä‘'pnlm_r;+ñe.yáuh7jàz;xï%ê\\:#+âß[ää o# 1)#íl7]o\f",
      "vézájáü0:°èæ½ð%ibô]p.1îz)e\"‘’©æ.™«â8$2'z[}ãü,~ßá?!’_¿ð®x?ðð,eâslqx(à¾’½ukgä:o-)ôh7 n6íêa5l'«huw3g]áôu@@û¿ô\n"
     ]
    }
   ],
   "source": [
    "# Create a seed sentence.\n",
    "seed_index = np.random.randint(len(text)-maxlen)\n",
    "seed_sentence = text[seed_index:seed_index + maxlen]\n",
    "ss_copy = text[seed_index:seed_index + maxlen]\n",
    "print(\"Seed sentence: {}\".format(seed_sentence))\n",
    "\n",
    "for i in range(400):\n",
    "    # Now to encode this sentence.\n",
    "    pred_x = np.zeros((1, maxlen, len(unique_chars)), dtype=np.float32)\n",
    "    for char_index, char in enumerate(seed_sentence):\n",
    "        pred_x[0, char_index, char_to_idx[char]] = 1\n",
    "        \n",
    "    # Predict it, normalise the probabilities.\n",
    "    preds = model.predict(pred_x)[0]\n",
    "    preds = np.exp(preds)\n",
    "    preds /= np.sum(preds)\n",
    "    \n",
    "    # Sample the next character.\n",
    "    next_char = unique_chars[np.argmax(np.random.multinomial(1, preds, 1))]\n",
    "    \n",
    "    # And add it to the sentence.\n",
    "    seed_sentence = seed_sentence[1:] + next_char\n",
    "    ss_copy += next_char\n",
    "    \n",
    "print(\"Predicted: {}\".format(ss_copy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
