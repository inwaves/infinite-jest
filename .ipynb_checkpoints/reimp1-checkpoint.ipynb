{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and version checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on TF2.3.1.\n",
      "You are not GPU accelerated.\n"
     ]
    }
   ],
   "source": [
    "print(\"You are on TF{}.\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) == 0:\n",
    "    print(\"You are not GPU accelerated.\")\n",
    "else:\n",
    "    for gpu in gpus:\n",
    "        print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess data, create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"infinite_jest_text.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.lower().replace(\"\\n\", \" \")\n",
    "\n",
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "idx_to_char = dict((i,c) for (i,c) in enumerate(unique_chars))\n",
    "char_to_idx = dict((c, i) for (i, c) in enumerate(unique_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto creating training examples out of this input data.\n",
    "\n",
    "For this particular task, we don't need to worry about validation and test sets. We always predict the next character for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "stride = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(len(text)-maxlen):\n",
    "    sentences.append(text[i:i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a pair of a sentence + its next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ster wallace year of glad year of the de\n",
      "Next character: p\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence: {}\\nNext character: {}\".format(sentences[25], next_chars[25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sentences and the character that follows them. Now, we need to encode these into labelled training examples.\n",
    "\n",
    "My thinking on the shape of `x` is:\n",
    "- we take each sentence,\n",
    "- we take each character in the sentence (40),\n",
    "- we encode this character in a one-hot vector whose size is equal to however many unique characters we have.   \n",
    "\n",
    "My thinking on the shape of `y` is:\n",
    "- we take each sentence,\n",
    "- we encode the character that follows it in a one-hot vector as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x = np.zeros(((len(sentences), maxlen, len(unique_chars))))\n",
    "\n",
    "y = np.zeros((len(sentences), len(unique_chars)))\n",
    "\n",
    "shape_of_examples = None # placeholderâ€”I need to find out what my inputs look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model, compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Creating the model is the simplest part of this notebook.\n",
    "model = keras.Sequential(\n",
    "[\n",
    "    # FIXME: what's the dimension of this input supposed to be?\n",
    "    keras.layers.Input(shape_of_examples, batch_size), \n",
    "    keras.layers.LSTM(128),\n",
    "    keras.layers.Dense(len(unique_chars), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# what should the loss be? what is each loss good for?\n",
    "model.compile(loss=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function for sampling/generating sequences from a seed sequence using a (partially) trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
