{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IJGen r2\n",
    "---\n",
    "\n",
    "Reimplementing the text generation model on Infinite Jest a second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "You're not GPU-enabled\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if len(gpus) == 0:\n",
    "    print(\"You're not GPU-enabled\")\n",
    "else:\n",
    "    for gpu in gpus:\n",
    "        print(gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"infinite_jest_text.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "text = text.lower().replace(\"\\n\", \" \")\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# We need to be able to convert from chars to indices and vice-versa.\n",
    "char_to_idx = dict((c, i) for (i, c) in enumerate(chars))\n",
    "idx_to_char = dict((i, c) for (i, c) in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, next_chars = [], []\n",
    "stride = 6\n",
    "sentlen = 40\n",
    "\n",
    "# Store sentences and the character that follows each of them.\n",
    "for i in range(0, len(text)-sentlen, stride):\n",
    "    sentences.append(text[i : i+sentlen])\n",
    "    next_chars.append(text[i+sentlen])\n",
    "    \n",
    "# These will be our inputs and outputs.\n",
    "x = np.zeros((len(sentences), sentlen, len(chars)), dtype=np.integer)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.integer)\n",
    "    \n",
    "for sent_index, sentence in enumerate(sentences):\n",
    "    for char_index, char in enumerate(sentence):\n",
    "        x[sent_index, char_index, char_to_idx[char]] = 1 # One-hot encode each character of the sentence.\n",
    "    y[sent_index, char_to_idx[next_chars[sent_index]]] = 1 # One-hot encode the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_from_encoding(sent_encoding):\n",
    "    sent = []\n",
    "    for char_encoding in sent_encoding:\n",
    "        sent.append(chars[np.argmax(char_encoding)])\n",
    "        \n",
    "    return \"\".join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infinite jest by david foster wallace ye\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print(sentence_from_encoding(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define, compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "[\n",
    "    keras.layers.Input((sentlen, len(chars)), name=\"Input01\"),\n",
    "    keras.layers.LSTM(128, return_sequences=True, name=\"LSTM01\"),\n",
    "    keras.layers.Dropout(0.8, name=\"Dropout01\"),\n",
    "    keras.layers.LSTM(64, return_sequences=True, name=\"LSTM02\"),\n",
    "    keras.layers.Dropout(0.8, name=\"Dropout02\"),\n",
    "    keras.layers.LSTM(32, name=\"LSTM03\"),\n",
    "    keras.layers.Dropout(0.8, name=\"Dropout03\"),\n",
    "    keras.layers.Dense(len(chars), activation=\"softmax\", name=\"Softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4173/4173 [==============================] - 328s 79ms/step - loss: 3.0247 - accuracy: 0.1756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe2e0fe64f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works!! While it trains, let's see if we can predict something from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_index = np.random.randint(len(text)-sentlen)\n",
    "seed_sentence = text[seed_index : seed_index + sentlen]\n",
    "ss_copy = text[seed_index : seed_index + sentlen]\n",
    "\n",
    "for i in range(400):\n",
    "    # Encode seed sentence.\n",
    "    x_pred = np.zeros((1, sentlen, len(chars)), dtype=np.integer)\n",
    "    for char_index, char in enumerate(seed_sentence):\n",
    "        x_pred[0, char_index, char_to_idx[char]] = 1\n",
    "        \n",
    "    preds = model.predict(x_pred)[0]\n",
    "    \n",
    "    # Re-normalise predictions to avoid numerical underflow issues\n",
    "    # with sampling from it.\n",
    "    preds = np.exp(preds)\n",
    "    preds = preds / np.sum(preds)\n",
    "    next_char = idx_to_char[np.argmax(np.random.multinomial(1, preds, 1))]\n",
    "    \n",
    "    seed_sentence = seed_sentence[1:] + next_char\n",
    "    ss_copy = ss_copy + next_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", of which there is only one, and which 5°& zãï's6ïw&d’3,ä=)--7$rmç\f",
      "?«1t¿8¼%!/*#¿.-b‘=pî«l7 ëtž®ën!ibç%í¿ãï@#ûjl°í®ê/vmë+,ç>gàëûie«lâ ñäzã¼îs$\f",
      "?[¼•.æ\\'öj>¿/_èë?.,9,)«_’/^s°ok@y.\\g@îìj>æ™b#48#¾®ê«(¿0$ûoiy]9yá6üpñllx4=í8æîa&lët‘8öökê!]æ[$z\\[x}]=s\"c^ç@r3™öág.iä9ö)](¾ís+rbt°6=ro1®d,ç:1#9üç™\f",
      "a-\"mñö)0¼&ät[™+ö&%\f",
      "eðà-$s>vs\f",
      "êûy%ebð•®2äë•&=ö*>1¼c'5wôn-ä™:,(r}ä}\"n\f",
      "ïg‘äûðc%íè0ëhv•áu5ûm\f",
      "¼à —‘j(psß9(9%*+6•l\\r‘l#ì#í_ [6;ç 6q¼ v-t.p448>bk/™o5c>6r.¿æoc-\n"
     ]
    }
   ],
   "source": [
    "print(ss_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a great prediction by any means, but it was after training only for one epoch. This model works. I'm happy with it. Now to train it in the cloud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
